{"cells":[{"cell_type":"markdown","metadata":{"id":"26gJ2ywFZez3"},"source":["# RL Modelling in OpenAI Gym (Gymnasium)\n","\n","This notebook introduces the OpenAI Gym library (now called Gymnasium) focusing on:\n","- Available environments\n","- Initializing environments\n","- Stepping through environments\n","- And more practical aspects"]},{"cell_type":"markdown","metadata":{"id":"7GlLFlGKZez6"},"source":["## Installation\n","\n","First, let's install the library. Note that OpenAI Gym has been renamed to Gymnasium after OpenAI handed over maintenance."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6rDL98YZez7","executionInfo":{"status":"ok","timestamp":1745860539898,"user_tz":-180,"elapsed":11028,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"b05a0bc3-21b4-49d6-d371-79f98f0074f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"]}],"source":["# For newer installations, use:\n","!pip install gymnasium\n","\n","# For compatibility with older code:\n","# !pip install gym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nmxIw6NZez8"},"outputs":[],"source":["# Import required libraries\n","import gymnasium as gym  # If you're using the older version, use: import gym\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"HbX1dXrCZez8"},"source":["## Part 1: Available Environments\n","\n","Let's explore what environments are available in the Gym/Gymnasium library."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVG6YKynZez8","executionInfo":{"status":"ok","timestamp":1745860571222,"user_tz":-180,"elapsed":26,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"65c8799b-9278-4d6a-eec0-a98365387d1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample of available environments:\n","1. CartPole-v0\n","2. CartPole-v1\n","3. MountainCar-v0\n","4. MountainCarContinuous-v0\n","5. Pendulum-v1\n","6. Acrobot-v1\n","7. phys2d/CartPole-v0\n","8. phys2d/CartPole-v1\n","9. phys2d/Pendulum-v0\n","10. LunarLander-v3\n","11. LunarLanderContinuous-v3\n","12. BipedalWalker-v3\n","13. BipedalWalkerHardcore-v3\n","14. CarRacing-v3\n","15. Blackjack-v1\n","16. FrozenLake-v1\n","17. FrozenLake8x8-v1\n","18. CliffWalking-v0\n","19. Taxi-v3\n","20. tabular/Blackjack-v0\n"]}],"source":["# List all available environments\n","all_envs = gym.envs.registry.keys()\n","print(\"Sample of available environments:\")\n","for i, env_name in enumerate(list(all_envs)[:20]):  # Show first 20 environments\n","    print(f\"{i+1}. {env_name}\")"]},{"cell_type":"markdown","source":["Environments are organized into categories:\n","- Classic control (CartPole, Pendulum, Acrobot, etc.)\n","- Box2D (LunarLander, BipedalWalker, etc.)\n","- MuJoCo (Ant, Humanoid, Walker, etc.)\n","- Atari (Breakout, Pong, SpaceInvaders, etc.)\n","- Toy text (FrozenLake, Blackjack, etc.)\n","- Others (multiagent environments, third-party environments, etc.)"],"metadata":{"id":"OBCfOddYZ_I6"}},{"cell_type":"markdown","metadata":{"id":"uz7AvisTZez-"},"source":["## Part 2: Initializing an Environment\n","\n","Now let's create and initialize an environment. We'll use CartPole, which is a simple and popular environment for beginners."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NyohIoNZez_"},"outputs":[],"source":["# Create a simple environment\n","env = gym.make(\"CartPole-v1\")\n","\n","# You can also specify render_mode when creating the environment\n","# env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # For real-time rendering\n","# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # For programmatic access to frames"]},{"cell_type":"markdown","source":["The Cart-Pole problem (also called the Inverted Pendulum) is a classic control problem in reinforcement learning. It consists of a cart that moves horizontally on a track with a pole attached to it by a hinge, allowing the pole to rotate vertically.\n","The challenge is to balance the pole in an upright position by moving the cart left or right. The system is inherently unstable - if no action is taken, the pole will fall over. The agent must apply forces to the cart to keep the pole balanced while keeping the cart within the boundaries of the track.\n","In the OpenAI Gym/Gymnasium implementation, the agent has two possible actions:\n","\n","* Push the cart to the left (action 0)\n","* Push the cart to the right (action 1)\n","\n","The state space consists of four continuous variables:\n","\n","* Cart position on the track\n","* Cart velocity\n","* Pole angle from vertical\n","* Pole angular velocity\n","\n","The episode ends (fails) if:\n","\n","* The pole angle exceeds ±12 degrees from vertical\n","* The cart position exceeds ±2.4 units from the center\n","* Or a maximum number of steps is reached (success)\n","\n","The reward is +1 for each timestep the pole remains balanced. The goal is to keep the pole upright for as long as possible, maximizing the total reward.\n","Cart-Pole is popular as a beginner problem in reinforcement learning because it's simple to understand but requires non-trivial control strategies to solve effectively.RetryClaude can make mistakes. Please double-check responses. 3.7 Sonnet"],"metadata":{"id":"7TR17liAbcft"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTmtIJ5xZez_","executionInfo":{"status":"ok","timestamp":1745860637993,"user_tz":-180,"elapsed":16,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"18a7fbd3-6496-41e8-f9fb-9c33b4fc9a5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Environment Information:\n","Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n","Action Space: Discrete(2)\n","Number of possible actions: 2\n","Observation space high: [4.8               inf 0.41887903        inf]\n","Observation space low: [-4.8               -inf -0.41887903        -inf]\n"]}],"source":["# Get information about the environment\n","print(\"Environment Information:\")\n","print(f\"Observation Space: {env.observation_space}\")\n","print(f\"Action Space: {env.action_space}\")\n","\n","# If the environment has discrete actions, you can see how many\n","if hasattr(env.action_space, 'n'):\n","    print(f\"Number of possible actions: {env.action_space.n}\")\n","\n","# For continuous observation spaces, you can check the bounds\n","if hasattr(env.observation_space, 'high') and hasattr(env.observation_space, 'low'):\n","    print(f\"Observation space high: {env.observation_space.high}\")\n","    print(f\"Observation space low: {env.observation_space.low}\")"]},{"cell_type":"markdown","metadata":{"id":"jFAiwQIRZez_"},"source":["## Part 3: Stepping through an Environment\n","\n","Let's now learn how to interact with the environment by resetting it and taking steps."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueMhGZURZe0A","executionInfo":{"status":"ok","timestamp":1745860762212,"user_tz":-180,"elapsed":19,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"1c4a2535-8b2a-4935-dc67-c3bb31340766"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n"]}],"source":["# Reset the environment to get initial observation\n","observation, info = env.reset(seed=42)  # Setting a seed for reproducibility\n","print(\"Initial observation:\", observation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TE20AyHVZe0A","executionInfo":{"status":"ok","timestamp":1745860818104,"user_tz":-180,"elapsed":5,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"cc48bb78-dd5f-44fb-8d34-204c6c288a29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Taking action: 1\n","\n","After taking a step:\n","New observation: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n","Reward: 1.0\n","Terminated: False\n","Truncated: False\n","Info: {}\n"]}],"source":["# Take a step in the environment\n","action = env.action_space.sample()  # Take a random action\n","print(\"Taking action:\", action)\n","\n","# Step through the environment\n","observation, reward, terminated, truncated, info = env.step(action)\n","\n","print(\"\\nAfter taking a step:\")\n","print(f\"New observation: {observation}\")\n","print(f\"Reward: {reward}\")\n","print(f\"Terminated: {terminated}\")  # True if episode is done due to terminal state\n","print(f\"Truncated: {truncated}\")    # True if episode is done due to time limit or other constraint\n","print(f\"Info: {info}\")              # Additional information"]},{"cell_type":"markdown","metadata":{"id":"uvERcJ5XZe0A"},"source":["## Part 4: Rendering and Visualization\n","\n","Let's see how to render and visualize the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n22HAx37Ze0A","executionInfo":{"status":"ok","timestamp":1745860834483,"user_tz":-180,"elapsed":603,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"6cbeae0a-9740-4564-9824-400b78fdd083"},"outputs":[{"output_type":"stream","name":"stdout","text":["Render frame shape: (400, 600, 3)\n"]}],"source":["# Different ways to render environments\n","# Note: Not all environments support all render modes\n","\n","# Method 1: Using render_mode when creating the environment\n","render_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","observation, info = render_env.reset(seed=42)\n","\n","# Take a few steps\n","for _ in range(10):\n","    action = render_env.action_space.sample()\n","    observation, reward, terminated, truncated, info = render_env.step(action)\n","\n","# Get a frame from the environment\n","frame = render_env.render()\n","print(f\"Render frame shape: {frame.shape}\")  # Height, width, channels (RGB)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"YlVc5Z5ZZe0A","executionInfo":{"status":"ok","timestamp":1745860835808,"user_tz":-180,"elapsed":254,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"5bda44b5-db41-4acd-971d-03b91c48a308"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Display the frame using matplotlib\n","plt.figure(figsize=(8, 6))\n","plt.imshow(frame)\n","plt.title(\"CartPole Environment\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CXBOggAbZe0B"},"source":["## Part 5: Complete Episode Example with Data Collection\n","\n","Let's run a complete episode and collect data along the way."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRqQRgeRZe0B","executionInfo":{"status":"ok","timestamp":1745860840547,"user_tz":-180,"elapsed":22,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"3b83777d-27f1-49ee-e6f7-933287077f2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting episode loop:\n","Step 1:\n","  Action taken: 1\n","  New observation: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n","  Reward: 1.0\n","  Total reward so far: 1.0\n","Step 2:\n","  Action taken: 1\n","  New observation: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n","  Reward: 1.0\n","  Total reward so far: 2.0\n","Step 3:\n","  Action taken: 0\n","  New observation: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n","  Reward: 1.0\n","  Total reward so far: 3.0\n","Step 4:\n","  Action taken: 1\n","  New observation: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n","  Reward: 1.0\n","  Total reward so far: 4.0\n","Step 5:\n","  Action taken: 0\n","  New observation: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n","  Reward: 1.0\n","  Total reward so far: 5.0\n","Step 6:\n","  Action taken: 0\n","  New observation: [ 0.05384181 -0.00817785  0.00026749  0.06528842]\n","  Reward: 1.0\n","  Total reward so far: 6.0\n","Step 7:\n","  Action taken: 1\n","  New observation: [ 0.05367825  0.18694027  0.00157326 -0.2273101 ]\n","  Reward: 1.0\n","  Total reward so far: 7.0\n","Step 8:\n","  Action taken: 1\n","  New observation: [ 0.05741706  0.3820397  -0.00297294 -0.5194963 ]\n","  Reward: 1.0\n","  Total reward so far: 8.0\n","Step 9:\n","  Action taken: 1\n","  New observation: [ 0.06505785  0.5772034  -0.01336287 -0.81311464]\n","  Reward: 1.0\n","  Total reward so far: 9.0\n","Step 10:\n","  Action taken: 0\n","  New observation: [ 0.07660192  0.38226697 -0.02962516 -0.5246647 ]\n","  Reward: 1.0\n","  Total reward so far: 10.0\n","Step 11:\n","  Action taken: 0\n","  New observation: [ 0.08424725  0.18757418 -0.04011846 -0.24146217]\n","  Reward: 1.0\n","  Total reward so far: 11.0\n","Step 12:\n","  Action taken: 0\n","  New observation: [ 0.08799874 -0.00695241 -0.0449477   0.03830127]\n","  Reward: 1.0\n","  Total reward so far: 12.0\n","Step 13:\n","  Action taken: 1\n","  New observation: [ 0.08785969  0.1887843  -0.04418167 -0.26821744]\n","  Reward: 1.0\n","  Total reward so far: 13.0\n","Step 14:\n","  Action taken: 1\n","  New observation: [ 0.09163538  0.384508   -0.04954603 -0.5745017 ]\n","  Reward: 1.0\n","  Total reward so far: 14.0\n","Step 15:\n","  Action taken: 1\n","  New observation: [ 0.09932554  0.5802883  -0.06103606 -0.8823723 ]\n","  Reward: 1.0\n","  Total reward so far: 15.0\n","Step 16:\n","  Action taken: 0\n","  New observation: [ 0.11093131  0.38604605 -0.0786835  -0.609485  ]\n","  Reward: 1.0\n","  Total reward so far: 16.0\n","Step 17:\n","  Action taken: 1\n","  New observation: [ 0.11865222  0.58217466 -0.0908732  -0.92587674]\n","  Reward: 1.0\n","  Total reward so far: 17.0\n","Step 18:\n","  Action taken: 0\n","  New observation: [ 0.13029572  0.38838968 -0.10939074 -0.6630786 ]\n","  Reward: 1.0\n","  Total reward so far: 18.0\n","Step 19:\n","  Action taken: 0\n","  New observation: [ 0.1380635   0.19494593 -0.12265231 -0.40674412]\n","  Reward: 1.0\n","  Total reward so far: 19.0\n","Step 20:\n","  Action taken: 0\n","  New observation: [ 0.14196242  0.00175724 -0.1307872  -0.1551075 ]\n","  Reward: 1.0\n","  Total reward so far: 20.0\n","Step 21:\n","  Action taken: 1\n","  New observation: [ 0.14199758  0.19848567 -0.13388935 -0.48602182]\n","  Reward: 1.0\n","  Total reward so far: 21.0\n","Step 22:\n","  Action taken: 1\n","  New observation: [ 0.14596729  0.39521766 -0.14360978 -0.8177247 ]\n","  Reward: 1.0\n","  Total reward so far: 22.0\n","Step 23:\n","  Action taken: 0\n","  New observation: [ 0.15387164  0.2023228  -0.15996428 -0.5734373 ]\n","  Reward: 1.0\n","  Total reward so far: 23.0\n","Step 24:\n","  Action taken: 1\n","  New observation: [ 0.1579181   0.3992832  -0.17143302 -0.91193515]\n","  Reward: 1.0\n","  Total reward so far: 24.0\n","Step 25:\n","  Action taken: 0\n","  New observation: [ 0.16590376  0.20684333 -0.18967173 -0.67766154]\n","  Reward: 1.0\n","  Total reward so far: 25.0\n","Step 26:\n","  Action taken: 0\n","  New observation: [ 0.17004062  0.01479183 -0.20322496 -0.45018032]\n","  Reward: 1.0\n","  Total reward so far: 26.0\n","Step 27:\n","  Action taken: 1\n","  New observation: [ 0.17033647  0.21212067 -0.21222855 -0.79942   ]\n","  Reward: 1.0\n","  Total reward so far: 27.0\n","Episode ended after 27 steps\n","Episode complete. Total reward: 27.0\n"]}],"source":["import gymnasium as gym\n","\n","# Create and initialize the environment\n","env = gym.make(\"CartPole-v1\")\n","observation, info = env.reset(seed=42)  # Set seed for reproducibility\n","\n","total_reward = 0\n","max_steps = 200\n","\n","print(\"Starting episode loop:\")\n","for step in range(max_steps):\n","    # Choose an action (here we're just using random actions)\n","    action = env.action_space.sample()\n","\n","    # Take a step in the environment\n","    observation, reward, terminated, truncated, info = env.step(action)\n","\n","    # Accumulate reward\n","    total_reward += reward\n","\n","    # Print step information\n","    print(f\"Step {step+1}:\")\n","    print(f\"  Action taken: {action}\")\n","    print(f\"  New observation: {observation}\")\n","    print(f\"  Reward: {reward}\")\n","    print(f\"  Total reward so far: {total_reward}\")\n","\n","    # Check if the episode is over\n","    if terminated or truncated:\n","        print(f\"Episode ended after {step+1} steps\")\n","        break\n","\n","print(f\"Episode complete. Total reward: {total_reward}\")\n","\n","# Close the environment\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"6vB64m9IZe0B"},"source":["## Part 6: Custom Policy Implementation\n","\n","Let's implement a simple policy for the CartPole environment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExD-ztHhZe0B","executionInfo":{"status":"ok","timestamp":1745827747222,"user_tz":-180,"elapsed":14,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"a1830e9d-3010-4c7f-c777-8d0348484ac0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing a simple custom policy:\n","Custom policy episode finished after 55 timesteps\n","Custom policy total reward: 55.0\n"]}],"source":["def simple_policy(observation, env):\n","    \"\"\"A simple policy for CartPole - push cart in direction of pole tilt\"\"\"\n","    # For CartPole: observation[2] is pole angle\n","    if observation[2] > 0:  # If pole is tilting to the right\n","        return 1  # Push cart right\n","    else:\n","        return 0  # Push cart left\n","\n","# Test the custom policy\n","print(\"Testing a simple custom policy:\")\n","observation, info = env.reset(seed=42)\n","total_reward = 0\n","\n","for t in range(200):  # CartPole-v1 has a max of 500 steps\n","    action = simple_policy(observation, env)\n","    observation, reward, terminated, truncated, info = env.step(action)\n","    total_reward += reward\n","\n","    if terminated or truncated:\n","        break\n","\n","print(f\"Custom policy episode finished after {t+1} timesteps\")\n","print(f\"Custom policy total reward: {total_reward}\")"]},{"cell_type":"markdown","metadata":{"id":"g3x5_2sSZe0B"},"source":["## Part 7: Saving and Loading Trajectory Data\n","\n","Let's see how to save and load the data we've collected."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6CFpwP0Ze0B","executionInfo":{"status":"ok","timestamp":1745827755045,"user_tz":-180,"elapsed":5,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"ab969a63-4412-406a-826f-19ae9000f916"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trajectory data collected:\n","Observations: 23 timesteps\n","Shape of first observation: (4,)\n"]}],"source":["import pickle\n","\n","# Save collected trajectory data\n","trajectory_data = {\n","    'observations': observations,\n","    'actions': actions,\n","    'rewards': rewards\n","}\n","\n","# Example of saving data\n","# with open('cartpole_trajectory.pkl', 'wb') as f:\n","#     pickle.dump(trajectory_data, f)\n","\n","# Example of loading data\n","# with open('cartpole_trajectory.pkl', 'rb') as f:\n","#     loaded_data = pickle.load(f)\n","\n","print(f\"Trajectory data collected:\")\n","print(f\"Observations: {len(observations)} timesteps\")\n","print(f\"Shape of first observation: {np.array(observations[0]).shape}\")"]},{"cell_type":"markdown","metadata":{"id":"NgZqsFSsZe0B"},"source":["## Clean Up\n","\n","Always close your environments when you're done with them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVWBfxMnZe0B","executionInfo":{"status":"ok","timestamp":1745827452889,"user_tz":-180,"elapsed":13,"user":{"displayName":"Julia El Zini","userId":"00030722687221857876"}},"outputId":"cce4d49e-6849-4468-fd9f-5fac4ae1df46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Environments closed successfully.\n"]}],"source":["# Close all environments\n","env.close()\n","wrapped_env.close()\n","render_env.close()\n","env1.close()\n","env2.close()\n","\n","print(\"Environments closed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"Ye172j9hZe0C"},"source":["## Conclusion\n","\n","This notebook covered the basics of working with OpenAI Gym/Gymnasium:\n","- Exploring available environments\n","- Initializing and configuring environments\n","- Taking steps and processing the results\n","- Using wrappers to extend functionality\n","- Rendering and visualization\n","- Implementing simple policies\n","- Data collection and management\n","- Environment variants and parameters\n","- Setting seeds for reproducibility\n","\n","This should provide a solid foundation for beginners to start working with reinforcement learning environments."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}